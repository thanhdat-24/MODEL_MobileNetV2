import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import os
import multiprocessing
from model import build_model, fine_tune_model
from data_preprocessing import train_generator, val_generator, test_generator, CLASS_NAMES, NUM_CLASSES
from config import EPOCHS, FINE_TUNE_EPOCHS, MODEL_SAVE_PATH, MODEL_TYPE, IMG_SIZE, BATCH_SIZE, FINE_TUNE_LAYERS
from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, ReduceLROnPlateau, TensorBoard
from datetime import datetime

# ‚úÖ S·ª≠ d·ª•ng GPU n·∫øu c√≥
def setup_gpu():
    gpu_devices = tf.config.list_physical_devices('GPU')
    if gpu_devices:
        print("‚úÖ S·ª≠ d·ª•ng GPU ƒë·ªÉ train")
        for gpu in gpu_devices:
            tf.config.experimental.set_memory_growth(gpu, True)
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y GPU, s·∫Ω train b·∫±ng CPU")

# H√†m ch√≠nh ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh
def train_model():
    # T·∫°o th∆∞ m·ª•c plots v√† logs n·∫øu ch∆∞a c√≥
    os.makedirs("plots", exist_ok=True)
    log_dir = os.path.join("logs", datetime.now().strftime("%Y%m%d-%H%M%S"))
    os.makedirs(log_dir, exist_ok=True)

    # ‚úÖ Callbacks n√¢ng cao ƒë·ªÉ t·ªëi ∆∞u h√≥a qu√° tr√¨nh hu·∫•n luy·ªán
    callbacks = [
        # D·ª´ng s·ªõm khi m√¥ h√¨nh kh√¥ng c·∫£i thi·ªán
        EarlyStopping(
            monitor='val_loss', 
            patience=10,  # TƒÉng patience l√™n 10
            restore_best_weights=True,
            verbose=1
        ),
        # L∆∞u m√¥ h√¨nh t·ªët nh·∫•t
        ModelCheckpoint(
            MODEL_SAVE_PATH, 
            save_best_only=True, 
            save_format='h5',
            verbose=1
        ),
        # T·ª± ƒë·ªông gi·∫£m learning rate khi kh√¥ng c·∫£i thi·ªán
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.2,
            patience=5,
            min_lr=1e-6,
            verbose=1
        ),
        # L∆∞u l·ªãch s·ª≠ hu·∫•n luy·ªán
        CSVLogger('training_log.csv'),
        # Th√™m TensorBoard ƒë·ªÉ c√≥ th·ªÉ theo d√µi qu√° tr√¨nh hu·∫•n luy·ªán
        TensorBoard(log_dir=log_dir, histogram_freq=1)
    ]

    print(f"üëâ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán MobileNetV2 v·ªõi {NUM_CLASSES} l·ªõp th·ª±c ph·∫©m")
    print(f"üëâ K√≠ch th∆∞·ªõc ·∫£nh: {IMG_SIZE}, Batch size: {BATCH_SIZE}")

    # üöÄ Hu·∫•n luy·ªán giai ƒëo·∫°n 1 - Feature extraction
    model, base_model = build_model()  # L·∫•y c·∫£ model v√† base_model
    print("üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh giai ƒëo·∫°n 1 (Feature Extraction)...")
    print(f"üëâ S·ªë epochs: {EPOCHS}")

    # Hi·ªÉn th·ªã th√¥ng tin m√¥ h√¨nh
    model.summary()

    # X√°c ƒë·ªãnh li·ªáu c√≥ n√™n s·ª≠ d·ª•ng multiprocessing hay kh√¥ng (tr√°nh tr√™n Windows)
    use_mp = False if os.name == 'nt' else True
    workers = 1 if os.name == 'nt' else 4
    
    if not use_mp:
        print("‚ö†Ô∏è Kh√¥ng s·ª≠ d·ª•ng multiprocessing tr√™n Windows ƒë·ªÉ tr√°nh l·ªói")
    
    history = model.fit(
        train_generator, 
        validation_data=val_generator,  # S·ª≠ d·ª•ng validation generator
        epochs=EPOCHS, 
        callbacks=callbacks,
        verbose=1,
        workers=workers,  # Gi·∫£m s·ªë worker tr√™n Windows
        use_multiprocessing=use_mp  # T·∫Øt multiprocessing tr√™n Windows
    )

    # L∆∞u l·ªãch s·ª≠ training phase 1
    history_dict = {k: [float(x) for x in v] for k, v in history.history.items()}
    df_history = pd.DataFrame(history_dict)
    df_history.to_csv('train_history_phase1.csv', index=False, encoding='utf-8-sig')
    print("‚úÖ ƒê√£ l∆∞u l·ªãch s·ª≠ train phase 1 ra file train_history_phase1.csv")

    print(f"‚úÖ M√¥ h√¨nh ƒë√£ l∆∞u t·∫°i {MODEL_SAVE_PATH}")

    # üîß Fine-tune giai ƒëo·∫°n 2
    print("\nüîß B·∫Øt ƒë·∫ßu Fine-tuning m√¥ h√¨nh...")
    print(f"üëâ S·ªë epochs fine-tuning: {FINE_TUNE_EPOCHS}")
    print(f"üëâ S·ªë l·ªõp ƒë∆∞·ª£c m·ªü kh√≥a: {FINE_TUNE_LAYERS}")

    model = fine_tune_model(model, base_model, unfreeze_layers=FINE_TUNE_LAYERS)
    history_fine = model.fit(
        train_generator, 
        validation_data=val_generator,  # S·ª≠ d·ª•ng validation generator 
        epochs=FINE_TUNE_EPOCHS, 
        callbacks=callbacks,
        verbose=1,
        workers=workers,  # Gi·∫£m s·ªë worker tr√™n Windows
        use_multiprocessing=use_mp  # T·∫Øt multiprocessing tr√™n Windows
    )

    # L∆∞u l·ªãch s·ª≠ fine-tuning
    history_fine_dict = {k: [float(x) for x in v] for k, v in history_fine.history.items()}
    df_history_fine = pd.DataFrame(history_fine_dict)
    df_history_fine.to_csv('train_history_finetune.csv', index=False, encoding='utf-8-sig')
    print("‚úÖ ƒê√£ l∆∞u l·ªãch s·ª≠ train fine-tune ra file train_history_finetune.csv")

    print(f"‚úÖ M√¥ h√¨nh ƒë√£ l∆∞u sau Fine-tuning t·∫°i {MODEL_SAVE_PATH}")

    # üîΩ G·ªçi h√†m v·∫Ω bi·ªÉu ƒë·ªì v√† ƒë√°nh gi√° m√¥ h√¨nh
    plot_training_curves(history, history_fine)
    evaluate_model(model, test_generator)

# H√†m v·∫Ω bi·ªÉu ƒë·ªì training curves
def plot_training_curves(history, history_fine, save_dir="plots"):
    plt.figure(figsize=(14, 6))
    plt.suptitle(f"Training Curves for {MODEL_TYPE}", fontsize=16)

    # Subplot 1: Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy (Phase 1)')
    plt.plot(history.history['val_accuracy'], label='Val Accuracy (Phase 1)')
    if history_fine and len(history_fine.history['accuracy']) > 0:
        # N·ªëi k·∫øt qu·∫£ t·ª´ phase 1 v√† fine-tuning ƒë·ªÉ c√≥ bi·ªÉu ƒë·ªì li√™n t·ª•c
        plt.plot(range(len(history.history['accuracy']), 
                     len(history.history['accuracy']) + len(history_fine.history['accuracy'])),
                     history_fine.history['accuracy'], 
                     label='Train Accuracy (Fine-tune)')
        plt.plot(range(len(history.history['val_accuracy']), 
                     len(history.history['val_accuracy']) + len(history_fine.history['val_accuracy'])),
                     history_fine.history['val_accuracy'], 
                     label='Val Accuracy (Fine-tune)')
    plt.title('Accuracy qua t·ª´ng epoch')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    # Subplot 2: Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss (Phase 1)')
    plt.plot(history.history['val_loss'], label='Val Loss (Phase 1)')
    if history_fine and len(history_fine.history['loss']) > 0:
        # N·ªëi k·∫øt qu·∫£ t·ª´ phase 1 v√† fine-tuning ƒë·ªÉ c√≥ bi·ªÉu ƒë·ªì li√™n t·ª•c
        plt.plot(range(len(history.history['loss']), 
                     len(history.history['loss']) + len(history_fine.history['loss'])),
                     history_fine.history['loss'], 
                     label='Train Loss (Fine-tune)')
        plt.plot(range(len(history.history['val_loss']), 
                     len(history.history['val_loss']) + len(history_fine.history['val_loss'])),
                     history_fine.history['val_loss'], 
                     label='Val Loss (Fine-tune)')
    plt.title('Loss qua t·ª´ng epoch')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # L∆∞u bi·ªÉu ƒë·ªì
    save_path = os.path.join(save_dir, "training_curves_combined.png")
    plt.tight_layout()
    plt.savefig(save_path)
    print(f"üìä ƒê√£ l∆∞u bi·ªÉu ƒë·ªì t·ªïng h·ª£p t·∫°i: {save_path}")
    plt.close()
    
    # V·∫Ω th√™m bi·ªÉu ƒë·ªì Learning Rate (n·∫øu c√≥)
    if 'lr' in history.history:
        plt.figure(figsize=(10, 6))
        lr_data = history.history['lr']
        if history_fine and 'lr' in history_fine.history:
            lr_data.extend(history_fine.history['lr'])
        plt.plot(lr_data)
        plt.title('Learning Rate')
        plt.xlabel('Epoch')
        plt.ylabel('Learning Rate')
        plt.yscale('log')
        plt.grid(True)
        save_path = os.path.join(save_dir, "learning_rate.png")
        plt.savefig(save_path)
        plt.close()
        print(f"üìä ƒê√£ l∆∞u bi·ªÉu ƒë·ªì learning rate t·∫°i: {save_path}")

# H√†m ƒë√°nh gi√° m√¥ h√¨nh
def evaluate_model(model, test_generator, save_dir="plots"):
    # ƒê√°nh gi√° m√¥ h√¨nh
    print("\nüìä ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test...")
    test_loss, test_acc = model.evaluate(test_generator, verbose=1)
    print(f"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}")
    
    # L∆∞u k·∫øt qu·∫£ ch√≠nh v√†o file
    with open('final_results.txt', 'w') as f:
        f.write(f"Test Accuracy: {test_acc:.4f}\n")
        f.write(f"Test Loss: {test_loss:.4f}\n")
        f.write(f"M√¥ h√¨nh: {MODEL_TYPE}\n")
        f.write(f"K√≠ch th∆∞·ªõc ·∫£nh: {IMG_SIZE}\n")
        f.write(f"S·ªë l·ªõp: {NUM_CLASSES}\n")
    
    # L·∫•y confusion matrix v√† class_labels
    y_true = test_generator.classes
    class_labels = list(test_generator.class_indices.keys())
    
    # T·∫°o d·ª± ƒëo√°n
    print("üîç T·∫°o d·ª± ƒëo√°n cho t·∫≠p test...")
    y_pred_probs = model.predict(test_generator)
    y_pred = np.argmax(y_pred_probs, axis=1)
    
    # T·∫°o confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    # Xu·∫•t confusion matrix ra file CSV
    df_cm = pd.DataFrame(cm, index=class_labels, columns=class_labels)
    df_cm.to_csv('confusion_matrix_full.csv', encoding='utf-8-sig')
    print("‚úÖ ƒê√£ l∆∞u confusion matrix ra file confusion_matrix_full.csv")

    # L·ªçc ra c√°c l·ªõp b·ªã nh·∫ßm nhi·ªÅu nh·∫•t
    mistakes = np.sum(cm, axis=1) - np.diag(cm)
    df_mistakes = pd.DataFrame({
        'class': class_labels,
        'mistakes': mistakes
    })
    df_mistakes = df_mistakes.sort_values(by='mistakes', ascending=False)
    df_mistakes.to_csv('top_mistake_classes.csv', index=False, encoding='utf-8-sig')
    print("‚úÖ ƒê√£ l∆∞u top l·ªõp b·ªã nh·∫ßm nhi·ªÅu nh·∫•t ra file top_mistake_classes.csv")

    # L·ªçc ra c√°c c·∫∑p l·ªõp b·ªã nh·∫ßm nhi·ªÅu nh·∫•t
    mistake_pairs = []
    for i in range(len(class_labels)):
        for j in range(len(class_labels)):
            if i != j and cm[i, j] > 0:
                mistake_pairs.append((class_labels[i], class_labels[j], cm[i, j]))
    mistake_pairs = sorted(mistake_pairs, key=lambda x: x[2], reverse=True)
    df_pairs = pd.DataFrame(mistake_pairs, columns=['Th·ª±c t·∫ø', 'D·ª± ƒëo√°n', 'S·ªë l·∫ßn nh·∫ßm'])
    df_pairs.to_csv('top_mistake_pairs.csv', index=False, encoding='utf-8-sig')
    print("‚úÖ ƒê√£ l∆∞u top c·∫∑p l·ªõp b·ªã nh·∫ßm nhi·ªÅu nh·∫•t ra file top_mistake_pairs.csv")

    # V·∫Ω l·∫°i confusion matrix cho top 10 l·ªõp b·ªã nh·∫ßm nhi·ªÅu nh·∫•t
    top_n = min(10, len(df_mistakes))
    top_classes_idx = [list(class_labels).index(c) for c in df_mistakes.head(top_n)['class']]
    cm_sub = cm[top_classes_idx, :][:, top_classes_idx]
    labels_sub = [class_labels[i] for i in top_classes_idx]

    plt.figure(figsize=(10, 8))
    sns.heatmap(cm_sub, annot=True, fmt='d', cmap='Blues',
                xticklabels=labels_sub, yticklabels=labels_sub)
    plt.xlabel('D·ª± ƒëo√°n')
    plt.ylabel('Th·ª±c t·∫ø')
    plt.title(f'Ma tr·∫≠n nh·∫ßm l·∫´n cho {top_n} l·ªõp b·ªã nh·∫ßm nhi·ªÅu nh·∫•t')
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'confusion_matrix_top10.png'))
    print(f"‚úÖ ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n cho top {top_n} l·ªõp b·ªã nh·∫ßm nhi·ªÅu nh·∫•t ra file {os.path.join(save_dir, 'confusion_matrix_top10.png')}")
    plt.close()

    # Xu·∫•t b√°o c√°o ph√¢n lo·∫°i
    report = classification_report(y_true, y_pred, target_names=class_labels, output_dict=True)
    df_report = pd.DataFrame(report).transpose()
    df_report.to_csv('classification_report.csv', encoding='utf-8-sig')
    print("‚úÖ ƒê√£ l∆∞u b√°o c√°o ph√¢n lo·∫°i ra file classification_report.csv")

# ƒêi·ªÉm v√†o ch√≠nh khi ch·∫°y script
if __name__ == "__main__":
    # C·∫ßn thi·∫øt cho multiprocessing tr√™n Windows
    multiprocessing.freeze_support()
    
    # Thi·∫øt l·∫≠p GPU
    setup_gpu()
    
    # Ch·∫°y qu√° tr√¨nh hu·∫•n luy·ªán
    train_model()