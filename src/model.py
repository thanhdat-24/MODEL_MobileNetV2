from keras.applications import MobileNetV2
from keras.models import Model
from keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization
from keras.optimizers import Adam
from config import IMG_SIZE, LEARNING_RATE, MODEL_TYPE, DROPOUT_RATE, FINE_TUNE_LAYERS, FINE_TUNE_LR
from data_preprocessing import NUM_CLASSES  # ‚úÖ l·∫•y t·ª´ data_preprocessing

def build_model():
    # S·ª≠ d·ª•ng pretrained MobileNetV2 l√†m base model
    base_model = MobileNetV2(weights="imagenet", include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))
    base_model.trainable = False  # ƒê√≥ng bƒÉng t·∫•t c·∫£ c√°c l·ªõp c·ªßa base model ·ªü giai ƒëo·∫°n 1

    # X√¢y d·ª±ng m√¥ h√¨nh v·ªõi ki·∫øn tr√∫c ph·ª©c t·∫°p h∆°n ƒë·ªÉ t·∫≠n d·ª•ng ·∫£nh 128x128
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = BatchNormalization()(x)
    
    # Th√™m l·ªõp dense ƒë·∫ßu ti√™n v·ªõi nhi·ªÅu node h∆°n
    x = Dense(256, activation="relu")(x)
    x = BatchNormalization()(x)
    x = Dropout(DROPOUT_RATE)(x)
    
    # Th√™m l·ªõp dense th·ª© hai
    x = Dense(128, activation="relu")(x)
    x = BatchNormalization()(x)
    x = Dropout(DROPOUT_RATE)(x)
    
    # L·ªõp output
    output = Dense(NUM_CLASSES, activation="softmax")(x)

    model = Model(inputs=base_model.input, outputs=output)
    
    # Compile m√¥ h√¨nh
    model.compile(
        optimizer=Adam(learning_rate=LEARNING_RATE), 
        loss="categorical_crossentropy", 
        metrics=["accuracy"]
    )
    
    return model, base_model

def fine_tune_model(model, base_model=None, unfreeze_layers=FINE_TUNE_LAYERS, fine_tune_lr=FINE_TUNE_LR):
    """
    Fine-tune model b·∫±ng c√°ch m·ªü kh√≥a m·ªôt s·ªë l·ªõp cu·ªëi c·ªßa base model.
    
    Args:
        model: M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán ·ªü giai ƒëo·∫°n 1
        base_model: Base model (MobileNetV2)
        unfreeze_layers: S·ªë l·ªõp c·∫ßn m·ªü kh√≥a cho fine-tuning (t·ª´ cu·ªëi l√™n)
        fine_tune_lr: Learning rate cho fine-tuning (th∆∞·ªùng nh·ªè h∆°n learning rate ban ƒë·∫ßu)
    
    Returns:
        model: M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh l·∫°i cho fine-tuning
    """
    print(f"üëâ Fine-tuning m√¥ h√¨nh v·ªõi {unfreeze_layers} l·ªõp cu·ªëi ƒë∆∞·ª£c m·ªü kh√≥a")
    print(f"üëâ Learning rate cho fine-tuning: {fine_tune_lr}")
    
    # M·ªü kh√≥a c√°c l·ªõp cu·ªëi c·ªßa base model
    if base_model:
        # ƒê√≥ng bƒÉng t·∫•t c·∫£ c√°c l·ªõp
        base_model.trainable = True
        
        # ƒê√≥ng bƒÉng c√°c l·ªõp ƒë·∫ßu, ch·ªâ m·ªü c√°c l·ªõp cu·ªëi
        for layer in base_model.layers[:-unfreeze_layers]:
            layer.trainable = False
            
        # Hi·ªÉn th·ªã s·ªë l·ªõp trainable v√† non-trainable
        trainable_count = sum(1 for layer in base_model.layers if layer.trainable)
        non_trainable_count = sum(1 for layer in base_model.layers if not layer.trainable)
        print(f"‚úÖ Base model: {trainable_count} l·ªõp trainable, {non_trainable_count} l·ªõp non-trainable")
    else:
        # Fallback n·∫øu kh√¥ng c√≥ base_model
        for layer in model.layers[-unfreeze_layers:]:
            if not isinstance(layer, BatchNormalization):
                layer.trainable = True

    # Compile l·∫°i m√¥ h√¨nh v·ªõi learning rate th·∫•p h∆°n
    model.compile(
        optimizer=Adam(learning_rate=fine_tune_lr), 
        loss="categorical_crossentropy", 
        metrics=["accuracy"]
    )
    
    return model